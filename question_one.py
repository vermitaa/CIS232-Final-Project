# -*- coding: utf-8 -*-
"""Question One.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NEYYcyRaKJzgdnhh5c7bN1DyKA13G738
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

print("="*70)
print("GOODREADS QUOTES + ZENQUOTES API")
print("="*70)
print("Scraping ONE webpage with 30+ quotes\n")

# PART 1: SCRAPE GOODREADS
print("Step 1: Scraping quotes from Goodreads...")

url = "https://www.goodreads.com/quotes"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')

print(f"Status code: {response.status_code}")
print(f"Page loaded: {len(response.text)} characters\n")

quotes = []
authors = []

# Look for quote text in div with class "quoteText"
quote_divs = soup.find_all('div', class_='quoteText')

print(f"Found {len(quote_divs)} quote divs")

for div in quote_divs:
    # Get the quote text (before the author tag)
    quote_text = div.get_text(separator=' ', strip=True)

    # Clean up - remove the "―" and author part
    if '―' in quote_text:
        quote_text = quote_text.split('―')[0].strip()

    # Remove quote marks
    quote_text = quote_text.strip('"').strip('"').strip('"')

    # Get author
    author_tag = div.find('span', class_='authorOrTitle')
    if author_tag:
        author = author_tag.text.strip()
    else:
        author = 'Unknown'

    if quote_text and len(quote_text) > 10:
        quotes.append(quote_text)
        authors.append(author)
        print(f"  {len(quotes)}. {quote_text[:60]}... - {author}")

    if len(quotes) >= 50:
        break

print(f"\n✓ Scraped {len(quotes)} quotes!")

if len(quotes) < 30:
    print(f"\n❌ Only scraped {len(quotes)} quotes (need 30+)")
    print("Website structure may be blocking or different.")
    exit()

# Create DataFrame 1
df1 = pd.DataFrame({
    'Quote Number': list(range(1, len(quotes) + 1)),
    'Quote': quotes,
    'Author': authors,
    'Source': ['Goodreads - Popular Quotes'] * len(quotes)
})

print("\nDF1 - Scraped Quotes:")
print(df1.head(10))
print(f"\nTotal records: {len(df1)}")

#  PART 2: USE QUOTABLE API

print("\n" + "="*70)
print("Step 2: Getting quote details from ZenQuotes API...")
print("(FREE API - NO KEY NEEDED!)")
print("="*70)

# Use ZenQuotes API (free, no key needed)
# Note: Can only get 5 quotes at a time, so we'll call it multiple times

api_quotes = []
api_authors = []
api_lengths = []
api_categories = []
api_ids = []

# Call API in batches of 5
num_calls = (len(quotes) + 4) // 5  # Round up

for batch in range(num_calls):
    try:
        # ZenQuotes returns array of 5 random quotes
        response = requests.get("https://zenquotes.io/api/quotes")

        if response.status_code == 200:
            data = response.json()

            for quote_data in data:
                if len(api_quotes) >= len(quotes):
                    break

                api_quotes.append(quote_data.get('q', 'N/A'))
                api_authors.append(quote_data.get('a', 'N/A'))
                api_lengths.append(len(quote_data.get('q', '')))
                api_categories.append(quote_data.get('h', 'N/A'))  # HTML for category
                api_ids.append(quote_data.get('i', 'N/A'))
        else:
            print(f"  API returned status {response.status_code}")
            # Fill with N/A for this batch
            for _ in range(min(5, len(quotes) - len(api_quotes))):
                api_quotes.append('N/A')
                api_authors.append('N/A')
                api_lengths.append(0)
                api_categories.append('N/A')
                api_ids.append('N/A')

    except Exception as e:
        print(f"  API error batch {batch+1}: {str(e)}")
        # Fill with N/A for this batch
        for _ in range(min(5, len(quotes) - len(api_quotes))):
            api_quotes.append('N/A')
            api_authors.append('N/A')
            api_lengths.append(0)
            api_categories.append('N/A')
            api_ids.append('N/A')

    print(f"  Retrieved {len(api_quotes)}/{len(quotes)} quotes from API...")
    time.sleep(1)  # Be nice to API

print(f"✓ Retrieved {len(api_quotes)} quotes from API!")

# Create DataFrame 2
df2 = pd.DataFrame({
    'Quote Number': list(range(1, len(api_quotes) + 1)),
    'API Quote': api_quotes,
    'API Author': api_authors,
    'Length (chars)': api_lengths,
    'Category': api_categories,
    'Quote ID': api_ids
})

print("\nDF2 - Quotable.io API Data:")
print(df2.head(10))

#  PART 3: STATISTICS

print("\n" + "="*70)
print("STATISTICS")
print("="*70)

print("\n- DF1 (Goodreads Quotes) Statistics -")
print(df1.describe())

print("\n--- DF2 (Quotable API Data) Statistics ---")
print(df2.describe())

#  PART 4: EXPORT

print("\n" + "="*70)
print("Step 3: Exporting to CSV...")
print("="*70)

df1.to_csv('goodreads_quotes.csv', index=False)
df2.to_csv('quotes_api_data.csv', index=False)

print("\n✓ SUCCESS! Files exported:")
print("  → goodreads_quotes.csv")
print("  → quotes_api_data.csv")

#  PART 5: SUMMARY

print("\n" + "="*70)
print("COMPLETE!")
print("="*70)

print(f"\nTotal quotes scraped: {len(df1)}")
print(f"API quotes retrieved: {len(df2)}")

if len(df1) > 0:
    print(f"\nMost quoted author: {df1['Author'].mode()[0] if not df1['Author'].mode().empty else 'N/A'}")

if len(df2) > 0 and df2['Length (chars)'].max() > 0:
    print(f"Average quote length: {df2['Length (chars)'].mean():.0f} characters")
    print(f"Longest quote: {df2['Length (chars)'].max()} characters")

print("\n✓ Scraped Goodreads.com")
print("✓ Used ZenQuotes.io API")
print(f"✓ {len(quotes)} quotes")
print("✓ 5 pieces of API data per quote")